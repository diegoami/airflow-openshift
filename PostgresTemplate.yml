apiVersion: v1
kind: Template
labels:
  template: airflow
message: |-
  To access the Airflow web ui, you should open the Route for the 'webserver' deployment.
  To upload/write the DAGs, you should open the Route for the 'jupyter' deployment.
metadata:
  annotations:
    description: Deploys Apache Airflow on Openshift. 

    
      Required variables - 'Application Name', 'Airflow Web UI Username', 'Airflow Web UI Password' and 'Jupyter Password'. Rest of the variables are optional and are filled/generated with default/auto values, if you do not explicitly provide a value.


      Dependencies - Before deployment you can list all required Python pip packages in the 'Python PIP Requirements' parameter, separated by whitespace. After deployment you can fill your Python pip requirements.txt contents in the 'requirements' config-map.


      By default, the setup deploys the Airflow webserver backed by 2 Celery workers.
      The configuration for the worker pods can be changed according to the Openshift Quota(Limit Range). To get more quota, you should contact Openshift admins.


      WARNING - This deployment setup is still in the experimental stage.
    iconClass: icon-datavirt
    openshift.io/display-name: Apache Airflow
    openshift.io/documentation-url: https://github.com/CSCfi/airflow-openshift
    openshift.io/support-url: https://www.csc.fi/contact-info
    openshift.io/long-description: Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. 
    
      The worksflows are defined as code, so that they become more maintainable, versionable, testable, and collaborative.
    
      Airflow is used to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. 
      
      The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.
    openshift.io/provider-display-name: CSC
    tags: python, data pipelines, orchestration platform
    template.openshift.io/bindable: "false"
  name: apache-airflow
  
objects:
- apiVersion: v1
  kind: DeploymentConfig
  metadata:
    labels:
      app: ${APPLICATION_NAME}
      template: postgresql-ephemeral-template
    name: postgresql
  spec:
    replicas: 1
    selector:
      name: postgresql
    strategy:
      type: Recreate
    template:
      metadata:
        labels:
          name: postgresql
      spec:
        containers:
        - env:
          - name: POSTGRESQL_USER
            valueFrom:
              secretKeyRef:
                key: database-user
                name: postgresql
          - name: POSTGRESQL_PASSWORD
            valueFrom:
              secretKeyRef:
                key: database-password
                name: postgresql
          - name: POSTGRESQL_DATABASE
            valueFrom:
              secretKeyRef:
                key: database-name
                name: postgresql
          image: registry.access.redhat.com/rhscl/postgresql-95-rhel7@sha256:de66da4812f0de42cee0bef65899d75f8b1a7440858271f133c8f73c80be663d
          imagePullPolicy: IfNotPresent
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            tcpSocket:
              port: 5432
            timeoutSeconds: 1
          name: postgresql
          ports:
          - containerPort: 5432
            protocol: TCP
          readinessProbe:
            exec:
              command:
              - /bin/sh
              - -i
              - -c
              - psql -h 127.0.0.1 -U $POSTGRESQL_USER -q -d $POSTGRESQL_DATABASE -c
                'SELECT 1'
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          resources:
            limits:
              memory: 1Gi
          securityContext:
            capabilities: {}
            privileged: false
          volumeMounts:
          - mountPath: /var/lib/pgsql/data
            name: postgresql-data
        dnsPolicy: ClusterFirst
        restartPolicy: Always
        schedulerName: default-scheduler
        securityContext: {}
        terminationGracePeriodSeconds: 30
        volumes:
        - name: postgresql-data
          persistentVolumeClaim:
            claimName: ${PERSISTENT_VOLUME_CLAIM_DB}
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
        - postgresql
        from:
          kind: ImageStreamTag
          name: postgresql:9.5
          namespace: openshift
      type: ImageChange
    - type: ConfigChange
    

parameters:
- description: Name of the Airflow application
  displayName: Application Name
  name: APPLICATION_NAME
  required: true
- description: Username for the Airflow web UI authentication
  displayName: Airflow Web UI Username
  name: AUTHENTICATION_USERNAME
  required: true
- description: Password for the Airflow web UI authentication
  displayName: Airflow Web UI Password
  name: AUTHENTICATION_PASSWORD
  required: true
- description: Password for accessing the Jupyter web interface used for writing/uploading DAGs
  displayName: Jupyter Password
  name: JUPYTER_PASSWORD
  required: true
- description: Number of Celery workers
  displayName: Number of Workers
  name: WORKER_COUNT
  value: "2"
- description: Celery worker CPU (check with your project limits)
  displayName: Worker CPU
  name: WORKER_CPU
  value: "2"
- description: Celery worker memory size (check with your project limits)
  displayName: Worker Memory
  name: WORKER_MEMORY
  value: "2Gi"
- description: Python PIP requirements needed for the DAGs, separated by whitespace
  displayName: Python PIP Requirements
  name: PIP_REQUIREMENTS
  value: "pandas scipy"
- description: Username for accessing the Flower web UI for Celery workers
  from: '[A-Z0-9]{12}'
  generate: expression
  name: FLOWER_USER
- description: Password for accessing the Flower web UI for Celery workers
  from: '[A-Z0-9]{12}'
  generate: expression
  name: FLOWER_PASSWORD
- description: PostgreSQL (Airflow metadata DB) host
  displayName: PostgreSQL Hostname
  name: POSTGRES_HOST
  value: postgresql
  required: true
- description: Username for PostgreSQL user that will be used for accessing the database
  displayName: PostgreSQL Connection Username
  from: 'user[A-Z0-9]{5}'
  generate: expression
  name: POSTGRESQL_USER
  required: true
- description: Password for the PostgreSQL connection user
  displayName: PostgreSQL Connection Password
  from: '[a-zA-Z0-9]{16}'
  generate: expression
  name: POSTGRESQL_PASSWORD
  required: true
- description: Database name for PostgreSQL database
  displayName: PostgreSQL Connection Database
  from: 'airflow[A-Z0-9]{5}'
  generate: expression
  name: POSTGRESQL_DATABASE
  required: true
- description: Redis Host (to avoid issues with default naming in OpenShift)
  displayName: Redis Host
  name: REDIS_HOST
  value: redis
  required: true
- description: Redis Port (to avoid issues with default naming in OpenShift)
  displayName: Redis Port
  name: REDIS_PORT
  value: "6379"
  required: true
- description: Password for Redis database
  displayName: Redis Connection Password
  from: '[A-Z0-9]{15}'
  generate: expression
  name: REDIS_PASSWORD
  required: true
- description: Airflow image link
  displayName: Airflow Image Link
  name: AIRFLOW_IMAGE
  value: docker-registry.rahti.csc.fi/airflow-image/airflow-os:latest
  required: true
- description: Attached PERSISTENT volume claim name for storing the DAGs
  displayName: PERSISTENT Volume Claim Name (DAGs)
  name: PERSISTENT_VOLUME_CLAIM_DAG
  value: air-dags-pvc
- description: Size of the pvc volume storing DAGs
  displayName: DAG Volume Storage Size
  name: PERSISTENT_VOLUME_CLAIM_DAG_SIZE
  value: "1Gi"
- description: Attached PERSISTENT volume claim name for storing the logs
  displayName: PERSISTENT Volume Claim Name (Logs)
  name: PERSISTENT_VOLUME_CLAIM_LOG
  value: air-logs-pvc
- description: Size of the pvc volume storing logs
  displayName: Logs Volume Storage Size
  name: PERSISTENT_VOLUME_CLAIM_LOG_SIZE
  value: "10Gi"
- description: Attached PERSISTENT volume claim name for storing metadata in PostgreSQL database
  displayName: PERSISTENT Volume Claim Name (DB)
  name: PERSISTENT_VOLUME_CLAIM_DB
  value: air-db-pvc
- description: Size of the metadata volume storage
  displayName: Metadata Volume Storage Size
  name: PERSISTENT_VOLUME_CLAIM_DB_SIZE
  value: "1Gi"
- description: Attached PERSISTENT volume claim name for storing temporary data across Celery workers
  displayName: PERSISTENT Volume Claim Name (Temp Storage for Workers)
  name: PERSISTENT_VOLUME_CLAIM_TMP_WORKER
  value: air-tmp-worker-pvc
- description: Size of the temporary data storage in Celery workers
  displayName: Metadata Volume Storage Size
  name: PERSISTENT_VOLUME_CLAIM_TMP_WORKER_SIZE
  value: "2Gi"
